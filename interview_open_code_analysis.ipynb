{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPLOAD AND TRANSFORM INTERVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4600ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI client setup\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Option 1 (recommended): read from env var\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Option 2 (manual, not recommended): paste a key here temporarily\n",
    "OPENAI_API_KEY = \"YOUR KEY HERE!\"\n",
    "\n",
    "# Try loading from .env if not already set\n",
    "if not OPENAI_API_KEY:\n",
    "    try:\n",
    "        from dotenv import load_dotenv  # pip install python-dotenv\n",
    "        load_dotenv()\n",
    "        OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"OPENAI_API_KEY is not set. Set it as an env var or in a local .env file.\"\n",
    "    )\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_txt_files():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_paths = filedialog.askopenfilenames(\n",
    "        title=\"Select Interview Text Files\",\n",
    "        filetypes=[(\"Text files\", \"*.txt\")]\n",
    "    )\n",
    "    return list(file_paths)\n",
    "\n",
    "file_paths = upload_txt_files()\n",
    "print(f\"Uploaded {len(file_paths)} file(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_transcript(path, interview_id=None):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "    # Find the start of the transcript section\n",
    "    try:\n",
    "        start_idx = lines.index(\"Transcript\")\n",
    "        lines = lines[start_idx + 1:]\n",
    "    except ValueError:\n",
    "        print(f\"'Transcript' section not found in: {path}\")\n",
    "        return pd.DataFrame(columns=[\"speaker\", \"text\", \"time\", \"interview_id\"])\n",
    "\n",
    "    records = []\n",
    "    i = 0\n",
    "    while i < len(lines) - 1:\n",
    "        # Match timestamp and speaker, e.g., 0:03 | Emiliano\n",
    "        match = re.match(r\"^(\\d{1,2}:\\d{2})\\s+\\|\\s+(.*)\", lines[i])\n",
    "        if match:\n",
    "            timestamp = match.group(1).strip()\n",
    "            speaker = match.group(2).strip()\n",
    "            text = lines[i + 1].strip()\n",
    "            records.append({\n",
    "                \"speaker\": speaker,\n",
    "                \"text\": text,\n",
    "                \"time\": timestamp,\n",
    "                \"interview_id\": interview_id\n",
    "            })\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1  # Skip lines that don't match the pattern\n",
    "\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "\n",
    "for path in file_paths:\n",
    "    interview_id = os.path.basename(path).replace(\".txt\", \"\")\n",
    "    df = parse_transcript(path, interview_id=interview_id)  # pass interview_id directly\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# Combine all interview transcripts into one DataFrame\n",
    "all_data = pd.concat(all_dfs, ignore_index=True)\n",
    "display(all_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the interviewer's responses\n",
    "interviewer_names = [\"YOUR NAME HERE\"]\n",
    "all_data_filtered = all_data[~all_data[\"speaker\"].str.lower().isin([n.lower() for n in interviewer_names])]\n",
    "\n",
    "display(all_data_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom save path\n",
    "save_path = \"YOUR PATH HERE!\"\n",
    "all_data_filtered.to_csv(save_path, index=False)\n",
    "print(f\"Saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633b58ed",
   "metadata": {},
   "source": [
    "# PROCESS INTERVIEWS AND CREATE OPEN CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your cleaned interview file\n",
    "file_path = save_path  # This should already be defined in your notebook\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter out short/irrelevant utterances\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"word_count\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "df_filtered = df[df[\"word_count\"] >= 5].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate token cost before running\n",
    "n_rows = len(df_filtered)\n",
    "avg_tokens_per_prompt = 150  # rough estimate (input + output)\n",
    "total_est_tokens = n_rows * avg_tokens_per_prompt\n",
    "price_per_1k_tokens = 0.03  # GPT-4 estimate (input + output)\n",
    "estimated_cost = (total_est_tokens / 1000) * price_per_1k_tokens\n",
    "\n",
    "print(f\"🧮 Estimated API cost for {n_rows} responses: ~{estimated_cost:.2f} USD ({total_est_tokens} tokens)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get open code and rationale\n",
    "def get_code_and_reason(text):\n",
    "    prompt = f\"\"\"\n",
    "You are a qualitative researcher. Your task is to assign a short open code (2–4 words) to the following interview response and explain why you chose it.\n",
    "\n",
    "Text:\n",
    "\\\"{text}\\\"\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "  \"code\": \"short open code\",\n",
    "  \"rationale\": \"brief explanation of why this code fits\"\n",
    "}}\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.4\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        parsed = json.loads(content)\n",
    "        return parsed.get(\"code\", \"\"), parsed.get(\"rationale\", \"\")\n",
    "    except Exception as e:\n",
    "        return \"ERROR\", f\"ERROR: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes, rationales = [], []\n",
    "for row in tqdm(df_filtered.itertuples(), total=len(df_filtered)):\n",
    "    code, rationale = get_code_and_reason(row.text)\n",
    "    codes.append(code)\n",
    "    rationales.append(rationale)\n",
    "\n",
    "df_filtered[\"code\"] = codes\n",
    "df_filtered[\"code_rationale\"] = rationales\n",
    "df_filtered.drop(columns=[\"word_count\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_codes = df_filtered[\"code\"].dropna().unique()\n",
    "code_list_str = \", \".join(unique_codes)\n",
    "\n",
    "cluster_prompt = f\"\"\"\n",
    "You are a qualitative analyst. Group the following open codes into 4–10 high-level themes.\n",
    "For each theme, provide a brief explanation and list the codes it contains.\n",
    "\n",
    "Respond in JSON format like this\n",
    "{{\n",
    "  \"Theme Name 1\": {{\n",
    "    \"rationale\": \"explanation of this theme\",\n",
    "    \"codes\": [\"code1\", \"code2\", ...]\n",
    "  }},\n",
    "  ...\n",
    "}}\n",
    "Codes:\n",
    "{code_list_str}\n",
    "\"\"\"\n",
    "\n",
    "theme_response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": cluster_prompt}],\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "theme_data = theme_response.choices[0].message.content\n",
    "themes_dict = json.loads(theme_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e05219",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_codes = df_filtered[\"code\"].dropna().unique()\n",
    "code_list_str = \", \".join(unique_codes)\n",
    "\n",
    "cluster_prompt = f\"\"\"\n",
    "You are a qualitative analyst. Group the following open codes into 4–10 high-level themes.\n",
    "For each theme, provide a brief explanation and list the codes it contains.\n",
    "\n",
    "Respond in JSON format like this\n",
    "{{\n",
    "  \"Theme Name 1\": {{\n",
    "    \"rationale\": \"explanation of this theme\",\n",
    "    \"codes\": [\"code1\", \"code2\", ...]\n",
    "  }},\n",
    "  ...\n",
    "}}\n",
    "Codes:\n",
    "{code_list_str}\n",
    "\"\"\"\n",
    "\n",
    "theme_response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": cluster_prompt}],\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "theme_data = theme_response.choices[0].message.content\n",
    "themes_dict = json.loads(theme_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_theme = {}\n",
    "code_to_theme_rationale = {}\n",
    "\n",
    "for theme, info in themes_dict.items():\n",
    "    rationale = info.get(\"rationale\", \"\")\n",
    "    for c in info.get(\"codes\", []):\n",
    "        code_to_theme[c.lower()] = theme\n",
    "        code_to_theme_rationale[c.lower()] = rationale\n",
    "\n",
    "df_filtered[\"theme\"] = df_filtered[\"code\"].apply(lambda x: code_to_theme.get(x.lower(), \"Unclustered\"))\n",
    "df_filtered[\"theme_rationale\"] = df_filtered[\"code\"].apply(lambda x: code_to_theme_rationale.get(x.lower(), \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧭 Print Themes, Rationales, and Assigned Codes\n",
    "print(\"\\n🧭 Themes & Rationales and Their Codes:\")\n",
    "assigned_codes = set()\n",
    "\n",
    "for theme, info in themes_dict.items():\n",
    "    codes = info.get(\"codes\", [])\n",
    "    rationale = info.get(\"rationale\", \"[No rationale provided]\")\n",
    "\n",
    "    if not codes:\n",
    "        print(f\"\\n⚠️ Skipping theme '{theme}' (no codes).\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🔹 {theme} ({len(codes)} codes):\\n  Rationale: {rationale}\")\n",
    "    for code in sorted(codes):\n",
    "        print(f\"   - {code}\")\n",
    "        assigned_codes.add(code.lower())  # normalize for matching\n",
    "\n",
    "# 🔍 Identify unassigned (unclustered) codes\n",
    "all_codes = set(df_filtered[\"code\"].dropna().str.lower().unique())\n",
    "unclustered_codes = all_codes - assigned_codes\n",
    "\n",
    "if unclustered_codes:\n",
    "    print(f\"\\n❌ Unclustered Codes ({len(unclustered_codes)}):\")\n",
    "    for code in sorted(unclustered_codes):\n",
    "        print(f\"   - {code}\")\n",
    "else:\n",
    "    print(\"\\n✅ All codes were clustered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"coded_transcripts_with_themes_and_rationale.csv\"\n",
    "df_filtered.to_csv(output_path, index=False)\n",
    "print(f\"\\n💾 Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE CLOSED CODES AND RE-PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_codebook = {\n",
    "        \"Experience with data connectors\": \"Refers to the interviewee's experience with data connectors, including their familiarity with different types of connectors and their ability to use them effectively.\",\n",
    "        \"Experience with tasks\": \"Refers to the interviewee's experience using tasks, including their familiarity with different types of tasks and their ability to use them effectively.\",\n",
    "        \"Pain-points or surprises during setup\": \"Refers to the interviewee's setting up data connectors, including any issues or challenges they have encountered during the setup process.\",\n",
    "        \"Expectations vs reality\": \"Refers to the interviewee's expectations and reality of using data connectors, including any differences between their expectations and the actual experience.\",\n",
    "        \"Data connectors and/or Tasks success metrics\": \"Refers to the interviewee's plan to measure the success of the usage of data connectors or tasks after the implementation. This includes how they decided to measure the success of the usage of data connectors or tasks after the implementation.\",\n",
    "        \"What have data connectors and/or tasks changed for the team or company\": \"Refers to the interviewee's observations on how the usage of data connectors or tasks has changed the team or company, including any positive or negative impacts.\",\n",
    "        \"Business case and ROI\": \"Refers to how the interviewee has created a business case to received resources from the company (for example, in terms of time or engineering or development resources) in order to implement data connectors or tasks, as well as the expectation of the ROI this implementation will bring.\",\n",
    "        \"Data connectors and Task reporting\": \"Refers to how the interviewee monitors the usage of data connectors or tasks, including any reporting or dashboards they have created to track the usage of data connectors or tasks, or lack thereof.\",\n",
    "        \"Experienced task problems or issues\": \"Refers to any issues the interviewee or the extended team has experienced setting up a task. This includes any issues with the setup interface, prompting, testing, etc.\",\n",
    "        \"Prompting experience\": \"Prompting is an important part of using data connectors or tasks, since the AI agent must know when to trigger them and what procedure to follow. This refers to the experience and challenges the interviewee or the extended team have faced creating prompts, as well as their perceived ability and skills in creating prompts.\",\n",
    "        \"Trust in data connectors and/or Tasks\":\"Refers to the interviewee's trust in the data connectors or tasks, including any concerns or doubts they have about the reliability or effectiveness of the data connectors or tasks, and in which scenarios or use cases they would not use them.\",\n",
    "        \"KPIs\": \"Refers to the interviewee's organisational KPIs when it comes to data connectors or tasks. Mainly, what the company is trying to achieve with data connectors or tasks in the long term.\",\n",
    "        \"Testing data connectors and/or Tasks\": \"Refers to the interviewee's experience testing data connectors or tasks, including any issues or challenges they have encountered during the testing process (for example, understanding why a task doesn't trigger, at what point data connectors are called and visibility of the logs).\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_tokens_per_prompt_closed = 200  # slightly longer than open coding\n",
    "est_tokens_closed = len(df_filtered) * avg_tokens_per_prompt_closed\n",
    "est_cost_closed = (est_tokens_closed / 1000) * 0.03  # gpt-4 estimated pricing\n",
    "\n",
    "print(f\"🧮 Estimated cost for closed coding: ~{est_cost_closed:.2f} USD ({est_tokens_closed} tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_closed_code(text, codebook):\n",
    "    code_options = \"\\n\".join([f'\"{k}\": {v}' for k, v in codebook.items()])\n",
    "    prompt = f\"\"\"\n",
    "You are a qualitative analyst specializing in customer interview research. Your job is to assign **one best-fit code** from the list below to this interview response.\n",
    "\n",
    "CONTEXT: Some context about the topic of the interviews.\n",
    "\n",
    "CODING INSTRUCTIONS:\n",
    "- Choose the code that best captures the PRIMARY theme of the response\n",
    "- If the response doesn't clearly fit any code, select \"None\"\n",
    "- Focus on the main insight or experience being shared\n",
    "- Consider both explicit statements and implied meanings\n",
    "\n",
    "Codes:\n",
    "{code_options}\n",
    "\n",
    "Response:\n",
    "\\\"{text}\\\"\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "  \"code\": \"selected code or None\",\n",
    "  \"rationale\": \"brief reason why this code fits\"\n",
    "}}\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        parsed = json.loads(content)\n",
    "        return parsed.get(\"code\", \"None\"), parsed.get(\"rationale\", \"\")\n",
    "    except Exception as e:\n",
    "        return \"ERROR\", f\"ERROR: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_closed_list = []\n",
    "rationale_closed_list = []\n",
    "\n",
    "for row in tqdm(df_filtered.itertuples(), total=len(df_filtered)):\n",
    "    code, rationale = assign_closed_code(row.text, closed_codebook)\n",
    "    code_closed_list.append(code)\n",
    "    rationale_closed_list.append(rationale)\n",
    "\n",
    "df_filtered[\"code_closed\"] = code_closed_list\n",
    "df_filtered[\"code_closed_rationale\"] = rationale_closed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique closed codes (excluding None/error)\n",
    "valid_codes = df_filtered[\"code_closed\"].dropna().unique()\n",
    "valid_codes = [c for c in valid_codes if c.lower() not in [\"none\", \"error\"]]\n",
    "codes_str = \", \".join(valid_codes)\n",
    "\n",
    "theme_prompt = f\"\"\"\n",
    "You are a qualitative researcher. Cluster the following codes into 4–8 higher-level themes.\n",
    "\n",
    "For each theme, provide:\n",
    "- A name\n",
    "- A rationale\n",
    "- The codes that belong in it\n",
    "\n",
    "Respond in JSON like this:\n",
    "{{\n",
    "  \"Theme Name\": {{\n",
    "    \"rationale\": \"why these codes belong together\",\n",
    "    \"codes\": [\"code1\", \"code2\", ...]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Codes:\n",
    "{codes_str}\n",
    "\"\"\"\n",
    "\n",
    "theme_response_closed = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": theme_prompt}],\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "theme_data_closed = theme_response_closed.choices[0].message.content\n",
    "themes_dict_closed = json.loads(theme_data_closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each closed code to its clustered theme + rationale\n",
    "code_to_theme_closed = {}\n",
    "code_to_theme_rationale_closed = {}\n",
    "\n",
    "for theme, info in themes_dict_closed.items():\n",
    "    rationale = info.get(\"rationale\", \"\")\n",
    "    for c in info.get(\"codes\", []):\n",
    "        code_to_theme_closed[c.lower()] = theme\n",
    "        code_to_theme_rationale_closed[c.lower()] = rationale\n",
    "\n",
    "df_filtered[\"theme_closed\"] = df_filtered[\"code_closed\"].apply(\n",
    "    lambda x: code_to_theme_closed.get(x.lower(), \"Unclustered\") if isinstance(x, str) else \"Unclustered\"\n",
    ")\n",
    "\n",
    "df_filtered[\"theme_closed_rationale\"] = df_filtered[\"code_closed\"].apply(\n",
    "    lambda x: code_to_theme_rationale_closed.get(x.lower(), \"\") if isinstance(x, str) else \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"📊 Rows processed: {len(df_filtered)}\")\n",
    "print(f\"🔖 Unique closed codes (excluding 'None'): {len(valid_codes)}\")\n",
    "print(f\"🧠 Themes from closed codes: {len(themes_dict_closed)}\")\n",
    "\n",
    "print(\"\\n🧭 Closed Code Themes:\")\n",
    "for theme, info in themes_dict_closed.items():\n",
    "    print(f\"\\n🔹 {theme} ({len(info['codes'])} codes):\\n  Rationale: {info['rationale']}\")\n",
    "    for code in sorted(info[\"codes\"]):\n",
    "        print(f\"   - {code}\")\n",
    "\n",
    "# Show unclustered closed codes\n",
    "all_codes_closed = set([c.lower() for c in valid_codes])\n",
    "assigned_closed = set(code_to_theme_closed.keys())\n",
    "unclustered_closed = all_codes_closed - assigned_closed\n",
    "\n",
    "if unclustered_closed:\n",
    "    print(f\"\\n❌ Unclustered closed codes ({len(unclustered_closed)}):\")\n",
    "    for c in sorted(unclustered_closed):\n",
    "        print(f\"   - {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_closed = \"coded_transcripts_with_closed_coding.csv\"\n",
    "df_filtered.to_csv(output_path_closed, index=False)\n",
    "print(f\"\\n💾 Saved to {output_path_closed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be770ad5",
   "metadata": {},
   "source": [
    "# CREATE REPORT ON CLOSED CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def generate_report(df):\n",
    "    # Filter out unclustered or NaN themes\n",
    "    themes = df[\"theme_closed\"].dropna().unique()\n",
    "    themes = [t for t in themes if t.lower() != \"unclustered\"]\n",
    "\n",
    "    theme_sections = []\n",
    "\n",
    "    for theme in themes:\n",
    "        rationale_series = df[df[\"theme_closed\"] == theme][\"theme_closed_rationale\"].dropna()\n",
    "        if rationale_series.empty:\n",
    "            continue\n",
    "        rationale = rationale_series.iloc[0]\n",
    "\n",
    "        # Use verbatim quotes (text) from the CSV\n",
    "        quotes_df = df[df[\"theme_closed\"] == theme][[\"text\", \"interview_id\"]].dropna()\n",
    "        if quotes_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Select 1–3 quotes randomly\n",
    "        quotes_sample = quotes_df.sample(n=min(3, len(quotes_df)), random_state=42)\n",
    "\n",
    "        quotes_text = \"\"\n",
    "        for _, row in quotes_sample.iterrows():\n",
    "            text = row[\"text\"].strip()  # leave as-is for CSV matching\n",
    "            interview_id = row[\"interview_id\"]\n",
    "            quotes_text += f'\"{text}\"\\nInterview ID: {interview_id}\\n\\n'\n",
    "\n",
    "        section = f\"\"\"### {theme}\n",
    "\n",
    "**Theme Description:** {rationale}\n",
    "\n",
    "**Supporting Quotes:**\n",
    "{quotes_text}\"\"\"\n",
    "        theme_sections.append(section)\n",
    "\n",
    "    if not theme_sections:\n",
    "        return \"⚠️ No valid themes with quotes and rationales were found.\"\n",
    "\n",
    "    theme_content = \"\\n\".join(theme_sections)\n",
    "\n",
    "    report_prompt = f\"\"\"\n",
    "You are a research analyst. Write a clear, professional report based on the following themes and **verbatim participant quotes** from user interviews.\n",
    "\n",
    "### Instructions:\n",
    "- Begin with an **Introduction** summarizing what this report covers.\n",
    "- For each **Theme**, use the exact quotes provided without paraphrasing.\n",
    "- Include the quotes in bullet form or as blockquotes, preserving their original wording.\n",
    "- For each quote, also include the **Interview ID** below it.\n",
    "- For each theme, also include a summary of the theme findings.\n",
    "- At the end, write a **Summary** that highlights overall takeaways.\n",
    "- Finally, offer **Recommendations** based on the main insights from all themes.\n",
    "\n",
    "Here are the verbatim themes and quotes:\n",
    "{theme_content}\n",
    "\"\"\"\n",
    "    return report_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate structured prompt\n",
    "report_prompt = generate_report(df_filtered)\n",
    "\n",
    "# Call GPT-4 to generate the report\n",
    "report_response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": report_prompt}],\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "report_text = report_response.choices[0].message.content\n",
    "\n",
    "# Print the result\n",
    "print(report_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_interview_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(\"📄 Report saved to final_interview_report.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
